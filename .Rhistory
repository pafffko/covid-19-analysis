type = "prob")
knnClasses <- predict(knn_fit,
newdata = testing)
knn_confMatrix <- caret::confusionMatrix(data = knnClasses, testing$outcome)
knn_confMatrix
knn_ROC <- roc(response = testing$outcome,
predictor = knnClasses_prob[, "died"],
levels = rev(levels(testing$outcome)))
plot(knn_ROC,
print.auc = TRUE,
col="black")
legend("bottomright",
col=c(par("fg"), "blue"),
legend = c("Empirical"),
lwd=2)
my_learning_curve_knn <- function(training_data,
test_data,
outcome = NULL,
trControl = NULL)
{
train_len <- nrow(training_data)
if(train_len < 2)
stop("Training data must have at least 2 data points.")
if(nrow(test_data) < 1)
stop("Test data must have at least 1 data points")
if(is.null(outcome))
stop("Please give a character string for the outcome column name.")
if(is.null(trControl))
stop("Please put the train controller.")
learnCurve <- data.frame(m = integer(train_len),
RMSE_type = character(train_len),
RMSE_value = integer(train_len))
j <- 0
tGrid <- expand.grid(k = 10)
for (i in c(seq(5,train_len, 20), train_len))
{
j <- j + 1
tr <- training_data[1:i,]
set.seed(17)
trainModel <- train(outcome ~ .,
data = tr,
method = "knn",
trControl = trControl,
tuneGrid = tGrid)
learnCurve$m[j] <- i
learnCurve$RMSE_type[j] <- "Training Error"
learnCurve$RMSE_value[j] <- rmse(tr$outcome,
predict(trainModel,
newdata = tr))
j <- j + 1
learnCurve$m[j] <- i
learnCurve$RMSE_type[j] <- "Testing Error"
learnCurve$RMSE_value[j] <- rmse(test_data$outcome,
predict(trainModel,
newdata = test_data))
}
learnCurve[1:j,]
}
knn_lmc <- my_learning_curve_knn(training_data = training,
test_data = testing,
outcome = "outcome",
trControl = ctrl)
knn_learning_curves <- ggplot(knn_lmc, aes(x = m)) +
geom_line(aes(y = RMSE_value, color = RMSE_type), size=2.0) +
theme_bw() +
labs(title = "Learning Curves",
x = "Examples",
y = "RMSE value",
color = "Learning Curve")
ggsave(filename = "k_nn_learning_curves.svg",
plot = knn_learning_curves,
device = "svg",
path = "plots")
ggplotly(knn_learning_curves)
set.seed(17)
svm_fit <- train(outcome ~ .,
data = training,
method = "svmRadial",
trControl = ctrl)
svmClasses <- predict(svm_fit,
newdata = testing)
svmClasses_prob <- predict(svm_fit,
newdata = testing,
type = "prob")
svm_confMatrix <-caret::confusionMatrix(testing$outcome,
svmClasses)
svm_confMatrix
svm_ROC <- roc(response = testing$outcome,
predictor = svmClasses_prob[, "died"],
levels = rev(levels(testing$outcome)))
plot(svm_ROC,
print.auc = TRUE,
col="black")
legend("bottomright",
col=c(par("fg"), "blue"),
legend = c("Empirical"),
lwd=2)
my_learning_curve_svm <- function(training_data,
test_data,
outcome = NULL,
trControl = NULL)
{
train_len <- nrow(training_data)
if(train_len < 2)
stop("Training data must have at least 2 data points.")
if(nrow(test_data) < 1)
stop("Test data must have at least 1 data points")
if(is.null(outcome))
stop("Please give a character string for the outcome column name.")
if(is.null(trControl))
stop("Please put the train controller.")
learnCurve <- data.frame(m = integer(train_len),
RMSE_type = character(train_len),
RMSE_value = integer(train_len))
j <- 0
tGrid <- expand.grid(k = 10)
for (i in c(seq(20,train_len, 20), train_len))
{
j <- j + 1
tr <- training_data[1:i,]
set.seed(17)
trainModel <- train(outcome ~ .,
data = tr,
method = "svmRadial",
trControl = ctrl)
learnCurve$m[j] <- i
learnCurve$RMSE_type[j] <- "Training Error"
learnCurve$RMSE_value[j] <- rmse(tr$outcome,
predict(trainModel,
newdata = tr))
j <- j + 1
learnCurve$m[j] <- i
learnCurve$RMSE_type[j] <- "Testing Error"
learnCurve$RMSE_value[j] <- rmse(test_data$outcome,
predict(trainModel,
newdata = test_data))
}
learnCurve[1:j,]
}
svm_lmc <- my_learning_curve_svm(training_data = training,
test_data = testing,
outcome = "outcome",
trControl = ctrl)
svm_learning_curves <- ggplot(svm_lmc, aes(x = m)) +
geom_line(aes(y = RMSE_value, color = RMSE_type), size=2.0) +
theme_bw() +
labs(title = "Learning Curves",
x = "Examples",
y = "RMSE value",
color = "Learning Curve")
ggsave(filename = "svm_learning_curves.svg",
plot = svm_learning_curves,
device = "svg",
path = "plots")
ggplotly(svm_learning_curves)
set.seed(17)
rf_fit <- train(outcome ~ .,
data = training,
method = "rf",
trControl = ctrl,
ntree=10)
rfClasses_prob <- predict(rf_fit,
newdata = testing,
type = "prob")
rfClasses <- predict(rf_fit,
newdata = testing)
rf_confMatrix <- caret::confusionMatrix(data = rfClasses, testing$outcome)
rf_confMatrix
rf_ROC <- roc(response = testing$outcome,
predictor = rfClasses_prob[, "died"],
levels = rev(levels(testing$outcome)))
plot(rf_ROC,
print.auc = TRUE,
col="black")
legend("bottomright",
col=c(par("fg"), "blue"),
legend = c("Empirical"),
lwd=2)
my_learning_curve_rf <- function(training_data,
test_data,
outcome = NULL,
trControl = NULL)
{
train_len <- nrow(training_data)
if(train_len < 2)
stop("Training data must have at least 2 data points.")
if(nrow(test_data) < 1)
stop("Test data must have at least 1 data points")
if(is.null(outcome))
stop("Please give a character string for the outcome column name.")
if(is.null(trControl))
stop("Please put the train controller.")
learnCurve <- data.frame(m = integer(train_len),
RMSE_type = character(train_len),
RMSE_value = integer(train_len))
tGrid
j <- 0
for (i in c(seq(5,train_len, 20), train_len))
{
j <- j + 1
tr <- training_data[1:i,]
set.seed(17)
trainModel <- train(outcome ~ .,
data = tr,
method = "rf",
trControl = trControl,
ntree=10)
learnCurve$m[j] <- i
learnCurve$RMSE_type[j] <- "Training Error"
learnCurve$RMSE_value[j] <- rmse(tr$outcome,
predict(trainModel,
newdata = tr))
j <- j + 1
learnCurve$m[j] <- i
learnCurve$RMSE_type[j] <- "Testing Error"
learnCurve$RMSE_value[j] <- rmse(test_data$outcome,
predict(trainModel,
newdata = test_data))
}
learnCurve[1:j,]
}
rf_mlc <- my_learning_curve_rf(training_data = training,
test_data = testing,
outcome = "outcome",
trControl = ctrl)
rf_learning_curves <- ggplot(rf_mlc, aes(x = m)) +
geom_line(aes(y = RMSE_value, color = RMSE_type), size=2.0) +
theme_bw() +
labs(title = "Learning Curves",
x = "Examples",
y = "RMSE value",
color = "Learning Curve")
ggsave(filename = "rf_learning_curves.svg",
plot = rf_learning_curves,
device = "svg",
path = "plots")
ggplotly(rf_learning_curves)
knn_confMatrix
svm_confMatrix
rf_confMatrix
plot(knn_ROC,
col="red")
plot(svm_ROC,
col = "green",
add = TRUE)
plot(rf_ROC,
col = "blue",
add = TRUE)
legend("bottomright",
col=c("red", "green", "blue"),
legend = c("k-NN", "SVM", "Random Forests"),
lwd=2)
knn_train_lmc <- knn_lmc %>%
filter(RMSE_type == "Training Error") %>%
mutate(method = "k-NN")
knn_test_lmc <- knn_lmc %>%
filter(RMSE_type == "Testing Error") %>%
mutate(method = "k-NN")
svm_train_lmc <- svm_lmc %>%
filter(RMSE_type == "Training Error") %>%
mutate(method = "SVM")
svm_test_lmc <- svm_lmc %>%
filter(RMSE_type == "Testing Error") %>%
mutate(method = "SVM")
rf_train_lmc <- rf_mlc %>%
filter(RMSE_type == "Training Error") %>%
mutate(method = "Random Forests")
rf_test_lmc <- rf_mlc %>%
filter(RMSE_type == "Testing Error") %>%
mutate(method = "Random Forests")
training_curves <- rbind(knn_train_lmc, svm_train_lmc, rf_train_lmc)
testing_curves <- rbind(knn_test_lmc, svm_test_lmc, rf_test_lmc)
comp_training_curves <- ggplot(training_curves, aes(x = m)) +
geom_line(aes(y = RMSE_value, color = method), size=2.0) +
theme_bw() +
labs(title = "Learning Curves for training",
x = "Examples",
y = "RMSE value",
color = "Method of learning")
ggsave(filename = "comparison_training_curves.svg",
plot = comp_training_curves,
device = "svg",
path = "plots")
ggplotly(comp_training_curves)
comp_testing_curves <- ggplot(testing_curves, aes(x = m)) +
geom_line(aes(y = RMSE_value, color = method), size=2.0) +
theme_bw() +
labs(title = "Learning Curves for testing",
x = "Examples",
y = "RMSE value",
color = "Method of learning")
ggsave(filename = "comparison_testing_curves.svg",
plot = comp_testing_curves,
device = "svg",
path = "plots")
ggplotly(comp_testing_curves)
attributes <- colnames(select(ml_df, -"outcome"))
importance_vector <- as.vector(rf_fit$finalModel$importance)
importance_df <- data.frame(matrix(c(attributes, importance_vector), ncol = 2))
colnames(importance_df) <- c("Attribute", "Importance")
importance_df <- importance_df %>%
arrange(desc(as.numeric(Importance)))
kable(importance_df)
most_important_attr_rf <- importance_df %>%
filter(Importance > 0)
most_important_attr_rf <- most_important_attr_rf[1:ceiling(nrow(most_important_attr_rf)),1]
new_training <- training %>%
select(c("outcome", all_of(most_important_attr_rf)))
new_testing <- testing %>%
select(c("outcome", all_of(most_important_attr_rf)))
set.seed(17)
new_rf_fit <- train(outcome ~ .,
data = new_training,
method = "rf",
trControl = ctrl,
ntree=10)
new_rfClasses_prob <- predict(new_rf_fit,
newdata = new_testing,
type = "prob")
new_rfClasses <- predict(new_rf_fit,
newdata = new_testing)
new_rf_confMatrix <- caret::confusionMatrix(data = new_rfClasses, new_testing$outcome)
new_rf_confMatrix
new_rf_ROC <- roc(response = new_testing$outcome,
predictor = new_rfClasses_prob[, "died"],
levels = rev(levels(new_testing$outcome)))
plot(new_rf_ROC,
print.auc = TRUE,
col="black")
legend("bottomright",
col=c(par("fg"), "blue"),
legend = c("Empirical"),
lwd=2)
new_rf_mlc <- my_learning_curve_rf(training_data = new_training,
test_data = new_testing,
outcome = "outcome",
trControl = ctrl)
tuned_model_learning_curves_1 <- ggplot(new_rf_mlc, aes(x = m)) +
geom_line(aes(y = RMSE_value, color = RMSE_type), size=2.0) +
theme_bw() +
labs(title = "Learning Curves",
x = "Examples",
y = "RMSE value",
color = "Learning Curve")
ggsave(filename = "tuned_model_learning_curves_1.svg",
plot = tuned_model_learning_curves_1,
device = "svg",
path = "plots")
ggplotly(tuned_model_learning_curves_1)
tuned_attr <- colnames(select(new_training, -"outcome"))
importance_vector2 <- as.vector(new_rf_fit$finalModel$importance)
importance_df2 <- data.frame(matrix(c(tuned_attr, importance_vector2), ncol = 2))
colnames(importance_df2) <- c("Attribute", "Importance")
importance_df2 <- importance_df2 %>%
arrange(desc(as.numeric(Importance)))
kable(importance_df2)
most_important_attr_rf2 <- importance_df2 %>%
filter(as.numeric(Importance) > 5) %>%
select(Attribute)
kable(most_important_attr_rf2)
most_important_attr_rf2 <- most_important_attr_rf2$Attribute
new_training2 <- training %>%
select(c("outcome", all_of(most_important_attr_rf2)))
new_testing2 <- testing %>%
select(c("outcome", all_of(most_important_attr_rf2)))
set.seed(17)
new_rf_fit2 <- train(outcome ~ .,
data = new_training2,
method = "rf",
trControl = ctrl,
ntree = 10)
new_rfClasses2 <- predict(new_rf_fit2,
newdata = new_testing2)
new_rfClasses2_prob <- predict(new_rf_fit2,
newdata = new_testing2,
type = "prob")
new_rf_confMatrix2 <- caret::confusionMatrix(data = new_rfClasses2,
new_testing2$outcome)
new_rf_confMatrix2
new_rf_ROC2 <- roc(response = new_testing2$outcome,
predictor = new_rfClasses2_prob[, "died"],
levels = rev(levels(new_testing2$outcome)))
plot(new_rf_ROC2,
print.auc = TRUE,
col="black")
legend("bottomright",
col=c(par("fg"), "blue"),
legend = c("Empirical"),
lwd=2)
new_rf_mlc2 <- my_learning_curve_rf(training_data = new_training2,
test_data = new_training2,
outcome = "outcome",
trControl = ctrl)
tuned_model_learning_curves_2 <- ggplot(new_rf_mlc2, aes(x = m)) +
geom_line(aes(y = RMSE_value, color = RMSE_type), size=2.0) +
theme_bw() +
labs(title = "Learning Curves",
x = "Examples",
y = "RMSE value",
color = "Learning Curve")
ggsave(filename = "tuned_model_learning_curves_2.svg",
plot = tuned_model_learning_curves_2,
device = "svg",
path = "plots")
ggplotly(tuned_model_learning_curves_2)
results <- data.frame(most_important_attr_rf2)
colnames(results) <- "Most important attributes"
kable(results)
tree_func <- function(final_model,
tree_num) {
#source: https://shiring.github.io/machine_learning/2017/03/16/rf_plot_ggraph
# get tree by index
tree <- randomForest::getTree(final_model,
k = tree_num,
labelVar = TRUE) %>%
tibble::rownames_to_column() %>%
# make leaf split points to NA, so the 0s won't get plotted
mutate(`split point` = ifelse(is.na(prediction), `split point`, NA))
# prepare data frame for graph
graph_frame <- data.frame(from = rep(tree$rowname, 2),
to = c(tree$`left daughter`, tree$`right daughter`))
# convert to graph and delete the last node that we don't want to plot
graph <- graph_from_data_frame(graph_frame) %>%
delete_vertices("0")
# set node labels
V(graph)$node_label <- gsub("_", " ", as.character(tree$`split var`))
V(graph)$leaf_label <- as.character(tree$prediction)
V(graph)$split <- as.character(round(tree$`split point`, digits = 2))
# plot
plot <- ggraph(graph, 'dendrogram') +
theme_bw() +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = node_label), na.rm = TRUE, repel = TRUE) +
geom_node_label(aes(label = split), vjust = 2.5, na.rm = TRUE, fill = "white") +
geom_node_label(aes(label = leaf_label, fill = leaf_label), na.rm = TRUE,
repel = TRUE, colour = "white", fontface = "bold", show.legend = FALSE) +
theme(panel.grid.minor = element_blank(),
panel.grid.major = element_blank(),
panel.background = element_blank(),
plot.background = element_rect(fill = "white"),
panel.border = element_blank(),
axis.line = element_blank(),
axis.text.x = element_blank(),
axis.text.y = element_blank(),
axis.ticks = element_blank(),
axis.title.x = element_blank(),
axis.title.y = element_blank(),
plot.title = element_text(size = 18))
print(plot)
}
tree <- tree_func(final_model = new_rf_fit2$finalModel,
tree_num = 5)
?plot
knn_ROC <- roc(response = testing$outcome,
predictor = knnClasses_prob[, "died"],
levels = rev(levels(testing$outcome)))
plot(knn_ROC,
print.auc = TRUE,
col="black")
legend("bottomright",
col=c(par("fg"), "blue"),
legend = c("Empirical"),
lwd=2)
knn_ROC <- roc(response = testing$outcome,
predictor = knnClasses_prob[, "died"],
levels = rev(levels(testing$outcome)))
plot(knn_ROC,
print.auc = TRUE,
col="black")
legend("bottomright",
col=c(par("fg"), "blue"),
legend = c("Empirical"),
lwd=5)
knn_ROC <- roc(response = testing$outcome,
predictor = knnClasses_prob[, "died"],
levels = rev(levels(testing$outcome)))
plot(knn_ROC,
print.auc = TRUE,
col="black",
lwd = 5)
legend("bottomright",
col=c(par("fg"), "blue"),
legend = c("Empirical"),
lwd=5)
plot(knn_ROC,
col="red")
plot(svm_ROC,
col = "green",
add = TRUE)
plot(rf_ROC,
col = "blue",
add = TRUE)
legend("bottomright",
col=c("red", "green", "blue"),
legend = c("k-NN", "SVM", "Random Forests"),
lwd = 5)
plot(knn_ROC,
col="red",
lwd = 5)
plot(svm_ROC,
col = "green",
add = TRUE,
lwd = 5)
plot(rf_ROC,
col = "blue",
add = TRUE,
lwd = 5)
legend("bottomright",
col=c("red", "green", "blue"),
legend = c("k-NN", "SVM", "Random Forests"),
lwd = 5)
